{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is a Python exploration of this R-based document: http://m-clark.github.io/data-processing-and-visualization/.  It is intended for those new to modeling and related concepts.  Code is *not* optimized for anything but learning.  In addition, all the content is located with the main document, not here, so many sections may not be included.  I only focus on reproducing the code chunks and providing some useful context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminary Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to suppress some output; I note where you can expect them.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>happiness_score</th>\n",
       "      <th>democratic_quality</th>\n",
       "      <th>generosity</th>\n",
       "      <th>healthy_life_expectancy_at_birth</th>\n",
       "      <th>log_gdp_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.625550</td>\n",
       "      <td>-1.950587</td>\n",
       "      <td>0.545034</td>\n",
       "      <td>-1.307413</td>\n",
       "      <td>7.500539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>11</td>\n",
       "      <td>-1.815967</td>\n",
       "      <td>-1.963219</td>\n",
       "      <td>0.314034</td>\n",
       "      <td>-1.333794</td>\n",
       "      <td>7.497038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>12</td>\n",
       "      <td>-1.431590</td>\n",
       "      <td>-1.998775</td>\n",
       "      <td>-0.687488</td>\n",
       "      <td>-1.360174</td>\n",
       "      <td>7.497755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Albania</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.399795</td>\n",
       "      <td>0.442664</td>\n",
       "      <td>-0.517345</td>\n",
       "      <td>0.618363</td>\n",
       "      <td>9.302960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Albania</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.669036</td>\n",
       "      <td>0.449130</td>\n",
       "      <td>-0.127154</td>\n",
       "      <td>0.657933</td>\n",
       "      <td>9.337532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country  year  happiness_score  democratic_quality  generosity  \\\n",
       "7   Afghanistan    10        -1.625550           -1.950587    0.545034   \n",
       "8   Afghanistan    11        -1.815967           -1.963219    0.314034   \n",
       "9   Afghanistan    12        -1.431590           -1.998775   -0.687488   \n",
       "18      Albania    10        -0.399795            0.442664   -0.517345   \n",
       "19      Albania    11        -0.669036            0.449130   -0.127154   \n",
       "\n",
       "    healthy_life_expectancy_at_birth  log_gdp_per_capita  \n",
       "7                          -1.307413            7.500539  \n",
       "8                          -1.333794            7.497038  \n",
       "9                          -1.360174            7.497755  \n",
       "18                          0.618363            9.302960  \n",
       "19                          0.657933            9.337532  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy = pd.read_csv('../data/world_hapiness.csv')\n",
    "\n",
    "def scale(x):\n",
    "    return((x - np.nanmean(x, axis=0))/np.nanstd(x, axis=0))\n",
    "\n",
    "happy_processed = happy[[\n",
    "    'country', \n",
    "    'year', \n",
    "    'happiness_score',\n",
    "    'democratic_quality',\n",
    "    'generosity',\n",
    "    'healthy_life_expectancy_at_birth',\n",
    "    'log_gdp_per_capita'\n",
    "]].apply(\n",
    "    lambda x: scale(x)\n",
    "    if x.name in [\n",
    "        'happiness_score',\n",
    "        'democratic_quality',\n",
    "        'generosity',\n",
    "        'healthy_life_expectancy_at_birth',] \n",
    "    else x\n",
    ").assign(\n",
    "    year = happy.year - np.min(happy.year)\n",
    ").dropna()\n",
    "\n",
    "happy_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [main document](http://m-clark.github.io/data-processing-and-visualization/ml.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python really shines with ML, and there are many more options there relative to classical statistical methods.  For standard ML, `scikit-learn` has long been a popular tool.  For deep learning and other techniques, many other modules are available that can take you far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A starting point for getting into ML from the more inferential methods is to use *regularized regression*.  These are conceptually no different than standard LM/GLM types of approaches, but they add something to the loss function.\n",
    "\n",
    "$$\\mathcal{Loss} = \\Sigma(y - \\hat{y})^2 + \\lambda\\cdot\\Sigma\\beta^2$$\n",
    "\n",
    "In the above, this is the same squared error loss function as before, but we add a penalty that is based on the size of the coefficients.  So, while initially our loss goes down with some set of estimates, the penalty based on their size might be such that the estimated loss actually increases. This has the effect of shrinking the estimates toward zero. Well, [why would we want that](https://stats.stackexchange.com/questions/179864/why-does-shrinkage-work)?  This introduces [bias in the coefficients](https://stats.stackexchange.com/questions/207760/when-is-a-biased-estimator-preferable-to-unbiased-one), but the end result is a model that will do better on test set prediction, which is the goal of the ML approach. The way this works regards the bias-variance tradeoff we discussed previously.  \n",
    "\n",
    "The following demonstrates regularized regression using the just the basic `statsmodels` approach. It actually uses *elastic net*, which has a mixture of two penalties, one of which is the squared sum of coefficients (typically called *ridge regression*, as above) and the other is the sum of their absolute values (the so-called *lasso*). The `L1_wt` varies the relative amount of lasso and ridge regression (1 = lasso, 0 = ridge). `alpha` is the actual penalty amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept            -0.066209\n",
       "democratic_quality    0.466727\n",
       "generosity            0.069540\n",
       "log_gdp_per_capita    0.012013\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_model_base = smf.ols(\n",
    "  'happiness_score ~ democratic_quality + generosity + log_gdp_per_capita',\n",
    "  data = happy_processed\n",
    ").fit_regularized(alpha = .2, L1_wt = .25)\n",
    "\n",
    "happy_model_base.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept            -5.703681\n",
       "democratic_quality    0.134756\n",
       "generosity            0.168902\n",
       "log_gdp_per_capita    0.613600\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smf.ols(\n",
    "  'happiness_score ~ democratic_quality + generosity + log_gdp_per_capita',\n",
    "  data = happy_processed\n",
    ").fit().params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we wouldn't know what alpha or the weight parameter should be.  This is where scikit-learn can help us use cross validation to choose those.  First lets split our data into training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = happy_processed.drop(columns=['happiness_score', 'country']) \n",
    "y = happy_processed[['happiness_score']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size = 0.33, \n",
    "    random_state = 1212\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a grid of values to run models via K-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "\n",
    "alphas = np.logspace(-10, -.5, 10)\n",
    "lambdas = np.linspace(0, 1, 10)\n",
    "\n",
    "tuned_parameters = [{'alpha': alphas, 'l1_ratio': lambdas}]\n",
    "n_folds = 10\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    ElasticNet(max_iter = 5e3, selection = 'random'), \n",
    "    tuned_parameters, \n",
    "    cv = n_folds, \n",
    "    refit = True,\n",
    "    scoring = 'neg_mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see available metrics for scoring\n",
    "# from sklearn.metrics import SCORERS\n",
    "# sorted(SCORERS.keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micl/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "cv_results = grid_search.fit(X_train, y_train)  # you will likely get some convergence warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reasons known only to the developers, rather than creating a simple directional flag, all scikit learn optimizes in a maximizing fashion, which means for something like mean squared error, it's actually dealing with negative values, and it doesn't bother to convert them to positive for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.297401498893825\n",
      "{'alpha': 0.02782559402207126, 'l1_ratio': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(cv_results.best_score_)   \n",
    "print(cv_results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get our prediction with the best model to get our test set error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.270871</td>\n",
       "      <td>0.659144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RMSE_test   R2_test\n",
       "0   0.270871  0.659144"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "pd.DataFrame({\n",
    "    'RMSE_test': mean_squared_error(cv_results.predict(X_test), y_test), \n",
    "    'R2_test': r2_score(cv_results.predict(X_test), y_test)}, \n",
    "    index = [0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A limitation of standard linear models, especially with many input variables, is that there's not a real automatic way to incorporate interactions and nonlinearities.  So we often will want to use techniques that do so.  To understand *random forests* and similar techniques (boosted trees, etc.), we can start with a simple decision tree.  To begin, for a single input variable (`X1`) whose range might be 1 to 10, we find that a cut at 5.75 results in the best classification, such that if all observations greater than or equal to 5.75 are classified as positive, and the rest negative. This general approach is fairly straightforward and conceptually easy to grasp, and it is because of this that tree approaches are appealing.\n",
    "\n",
    "Now letâ€™s add a second input (`X2`), also on a 1 to 10 range. We now might find that even better classification results if, upon looking at the portion of data regarding those greater than or equal to 5.75, that we only classify positive if they are also less than 3 on the second variable. The following is a hypothetical tree reflecting this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/tree1.png\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree structure allows for both interactions between variables, and nonlinear relationships between some input and the target variable (e.g. the second branch could just be the same `X1` but with some cut value greater than 5.75).  Random forests randomly select a few from the available input variables, and create a tree that minimizes (maximizes) some loss (objective) function on a validation set.  A given tree can potentially be very wide/deep, but instead of just one tree, we now do, say, 1000 trees. A final prediction is made based on the average across all trees. \n",
    "\n",
    "We demonstrate the random forest again using `scikit-learn`.  As before, we can tune some of the parameters, in this case, the number of predictors to randomly select each tree and how many observations should be in each leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "max_feature = [2, 3, X.shape[1]]\n",
    "min_samples_leaf = [1, 5, 10]\n",
    "\n",
    "tuned_parameters = [{'max_features': max_feature, 'min_samples_leaf': min_samples_leaf}]\n",
    "n_folds = 10\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(), \n",
    "    tuned_parameters, \n",
    "    cv = n_folds, \n",
    "    refit = True,\n",
    "    scoring = 'neg_mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micl/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "cv_results = grid_search.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19310426910754774\n",
      "{'max_features': 5, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cv_results.best_score_)   \n",
    "print(cv_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.234146</td>\n",
       "      <td>0.736937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RMSE_test   R2_test\n",
       "0   0.234146  0.736937"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'RMSE_test': mean_squared_error(cv_results.predict(X_test), y_test), \n",
    "    'R2_test': r2_score(cv_results.predict(X_test), y_test)}, \n",
    "    index = [0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see notable improvement over the regularized regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "*Neural networks* have been around for a long while as a general concept in artificial intelligence and even as a machine learning algorithm, and often work quite well. In some sense, neural networks can simply be thought of as nonlinear regression. Visually however, we can see them as a graphical model with layers of inputs and outputs. Weighted combinations of the inputs are created and put through some function (e.g. the sigmoid function) to produce the next layer of inputs. This next layer goes through the same process to produce either another layer, or to predict the output, or even multiple outputs, which serves as the final layer. All the layers between the input and output are usually referred to as hidden layers. If there were a single hidden layer with a single unit and no transformation, then it becomes the standard regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/nnet.png\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, we can run a simple neural network with a single hidden layer of up to 1000 units.  Since this is a regression problem, no further transformation is required of the end result to map it onto the target variable.  There are many tuning parameters I am not showing that could certainly be fiddled with as well. This is just an example that will run quickly with comparable performance to the previous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [500, 1000]  # size of single hidden layer\n",
    "lr = ['constant', 'adaptive']     # type of learning rate\n",
    "l2 = np.logspace(-4,-.1, 10)      # penalty parameter\n",
    "\n",
    "\n",
    "tuned_parameters = [{\n",
    "    'hidden_layer_sizes': hidden_layer_sizes, \n",
    "    'learning_rate': lr,\n",
    "    'alpha': l2\n",
    "}]\n",
    "n_folds = 10\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    MLPRegressor(max_iter = 1000), \n",
    "    tuned_parameters, \n",
    "    cv = n_folds, \n",
    "    refit = True,\n",
    "    scoring = 'neg_mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/micl/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "cv_results = grid_search.fit(X_train, y_train) # you get warnings but they can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2729463879488836\n",
      "{'alpha': 0.10797751623277094, 'hidden_layer_sizes': 1000, 'learning_rate': 'constant'}\n"
     ]
    }
   ],
   "source": [
    "print(cv_results.best_score_)   \n",
    "print(cv_results.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE_test</th>\n",
       "      <th>R2_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.26907</td>\n",
       "      <td>0.685276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RMSE_test   R2_test\n",
       "0    0.26907  0.685276"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'RMSE_test': mean_squared_error(cv_results.predict(X_test), y_test), \n",
    "    'R2_test': r2_score(cv_results.predict(X_test), y_test)}, \n",
    "    index = [0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Black Box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [main document](http://m-clark.github.io/data-processing-and-visualization/ml.html).  Also the [lime module](https://pypi.org/project/lime/), as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this has demystified the ML approach for you somewhat.  Nowadays it may take little effort to get to state-of-the-art results from even just a year or two ago, and which, for all intents and purposes, would be good enough both now and for the foreseeable future. Despite what many may think, it is not magic, but for more classically statistically minded, it may require a bit of a different perspective. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
